{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-8be54aeeaab9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msubprocess\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ls\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"../input/\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# Any results you write to the current directory are saved as output.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    625\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[1;32m--> 626\u001b[1;33m                **kwargs).stdout\n\u001b[0m\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(input, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    691\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stdin'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPIPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 693\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    694\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m             \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds)\u001b[0m\n\u001b[0;32m    945\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    946\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    948\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m             \u001b[1;31m# Cleanup if the child failed starting.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1222\u001b[0m                                          \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m                                          \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1224\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1225\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m                 \u001b[1;31m# Child is launched. Close the parent's copy of those pipe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import random\n",
    "import skimage.io\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import transform\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"../input/\"]).decode(\"utf8\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_image_labels(image_id):\n",
    "    # most of the content in this function is taken from 'Example Metric Implementation' kernel \n",
    "    # by 'William Cukierski'\n",
    "    image_file = \"../input/stage1_train/{}/images/{}.png\".format(image_id,image_id)\n",
    "    mask_file = \"../input/stage1_train/{}/masks/*.png\".format(image_id)\n",
    "    image = skimage.io.imread(image_file)\n",
    "    masks = skimage.io.imread_collection(mask_file).concatenate()    \n",
    "    height, width, _ = image.shape\n",
    "    num_masks = masks.shape[0]\n",
    "    labels = np.zeros((height, width), np.uint16)\n",
    "    for index in range(0, num_masks):\n",
    "        labels[masks[index] > 0] = index + 1\n",
    "    return image, labels\n",
    "\n",
    "def data_aug(image,label,angel=30,resize_rate=0.9):\n",
    "    flip = random.randint(0, 1)\n",
    "    size = image.shape[0]\n",
    "    rsize = random.randint(np.floor(resize_rate*size),size)\n",
    "    w_s = random.randint(0,size - rsize)\n",
    "    h_s = random.randint(0,size - rsize)\n",
    "    sh = random.random()/2-0.25\n",
    "    rotate_angel = random.random()/180*np.pi*angel\n",
    "    # Create Afine transform\n",
    "    afine_tf = transform.AffineTransform(shear=sh,rotation=rotate_angel)\n",
    "    # Apply transform to image data\n",
    "    image = transform.warp(image, inverse_map=afine_tf,mode='edge')\n",
    "    label = transform.warp(label, inverse_map=afine_tf,mode='edge')\n",
    "    # Randomly corpping image frame\n",
    "    image = image[w_s:w_s+size,h_s:h_s+size,:]\n",
    "    label = label[w_s:w_s+size,h_s:h_s+size]\n",
    "    # Ramdomly flip frame\n",
    "    if flip:\n",
    "        image = image[:,::-1,:]\n",
    "        label = label[:,::-1]\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_ids = check_output([\"ls\", \"../input/stage1_train/\"]).decode(\"utf8\").split()\n",
    "image_id = image_ids[random.randint(0,len(image_ids))]\n",
    "image, labels = read_image_labels(image_id)\n",
    "plt.subplot(221)\n",
    "plt.imshow(image)\n",
    "plt.subplot(222)\n",
    "plt.imshow(labels)\n",
    "\n",
    "new_image, new_labels = data_aug(image,labels,angel=5,resize_rate=0.9)\n",
    "plt.subplot(223)\n",
    "plt.imshow(new_image)\n",
    "plt.subplot(224)\n",
    "plt.imshow(new_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_data_augmentation(image_ids,split_num):\n",
    "    for ax_index, image_id in tqdm(enumerate(image_ids),total=len(image_ids)):\n",
    "        image,labels = read_image_labels(image_id)\n",
    "        if not os.path.exists(\"../input/stage1_train/{}/augs/\".format(image_id)):\n",
    "            os.makedirs(\"../input/stage1_train/{}/augs/\".format(image_id))\n",
    "        if not os.path.exists(\"../input/stage1_train/{}/augs_masks/\".format(image_id)):\n",
    "            os.makedirs(\"../input/stage1_train/{}/augs_masks/\".format(image_id))\n",
    "            \n",
    "        # also save the original image in augmented file \n",
    "        plt.imsave(fname=\"../input/stage1_train/{}/augs/{}.png\".format(image_id,image_id), arr = image)\n",
    "        plt.imsave(fname=\"../input/stage1_train/{}/augs_masks/{}.png\".format(image_id,image_id),arr = labels)\n",
    "\n",
    "        for i in range(split_num):\n",
    "            new_image, new_labels = data_aug(image,labels,angel=5,resize_rate=0.9)\n",
    "            aug_img_dir = \"../input/stage1_train/{}/augs/{}_{}.png\".format(image_id,image_id,i)\n",
    "            aug_mask_dir = \"../input/stage1_train/{}/augs_masks/{}_{}.png\".format(image_id,image_id,i)\n",
    "            plt.imsave(fname=aug_img_dir, arr = new_image)\n",
    "            plt.imsave(fname=aug_mask_dir,arr = new_labels)\n",
    "\n",
    "def clean_data_augmentation(image_ids):\n",
    "    for ax_index, image_id in tqdm(enumerate(image_ids),total=len(image_ids)):\n",
    "        if os.path.exists(\"../input/stage1_train/{}/augs/\".format(image_id)):\n",
    "            shutil.rmtree(\"../input/stage1_train/{}/augs/\".format(image_id))\n",
    "        if os.path.exists(\"../input/stage1_train/{}/augs_masks/\".format(image_id)):\n",
    "            shutil.rmtree(\"../input/stage1_train/{}/augs_masks/\".format(image_id))\n",
    "\n",
    "\n",
    "image_ids = check_output([\"ls\", \"../input/stage1_train/\"]).decode(\"utf8\").split()\n",
    "split_num = 10\n",
    "#make_data_augmentation(image_ids,split_num)\n",
    "#clean_data_augmentation(image_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_variable(name,shape):\n",
    "    return tf.get_variable(name, shape, initializer = tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "def UNet(X):\n",
    "    ### Unit 1 ###\n",
    "    with tf.name_scope('Unit1'):\n",
    "        W1_1 =   get_variable(\"W1_1\", [3,3,3,16] )\n",
    "        Z1 = tf.nn.conv2d(X,W1_1, strides = [1,1,1,1], padding = 'SAME')\n",
    "        A1 = tf.nn.relu(Z1)\n",
    "        W1_2 =   get_variable(\"W1_2\", [3,3,16,16] )\n",
    "        Z2 = tf.nn.conv2d(A1,W1_2, strides = [1,1,1,1], padding = 'SAME')\n",
    "        A2 = tf.nn.relu(Z2) \n",
    "        P1 = tf.nn.max_pool(A2, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "    ### Unit 2 ###\n",
    "    with tf.name_scope('Unit2'):\n",
    "        W2_1 =   get_variable(\"W2_1\", [3,3,16,32] )\n",
    "        Z3 = tf.nn.conv2d(P1,W2_1, strides = [1,1,1,1], padding = 'SAME')\n",
    "        A3 = tf.nn.relu(Z3)\n",
    "        W2_2 =   get_variable(\"W2_2\", [3,3,32,32] )\n",
    "        Z4 = tf.nn.conv2d(A3,W2_2, strides = [1,1,1,1], padding = 'SAME')\n",
    "        A4 = tf.nn.relu(Z4) \n",
    "        P2 = tf.nn.max_pool(A4, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "    ### Unit 3 ###\n",
    "    with tf.name_scope('Unit3'):\n",
    "        W3_1 =   get_variable(\"W3_1\", [3,3,32,64] )\n",
    "        Z5 = tf.nn.conv2d(P2,W3_1, strides = [1,1,1,1], padding = 'SAME')\n",
    "        A5 = tf.nn.relu(Z5)\n",
    "        W3_2 =   get_variable(\"W3_2\", [3,3,64,64] )\n",
    "        Z6 = tf.nn.conv2d(A5,W3_2, strides = [1,1,1,1], padding = 'SAME')\n",
    "        A6 = tf.nn.relu(Z6) \n",
    "        P3 = tf.nn.max_pool(A6, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "    ### Unit 4 ###\n",
    "    with tf.name_scope('Unit4'):\n",
    "        W4_1 =   get_variable(\"W4_1\", [3,3,64,128] )\n",
    "        Z7 = tf.nn.conv2d(P3,W4_1, strides = [1,1,1,1], padding = 'SAME')\n",
    "        A7 = tf.nn.relu(Z7)\n",
    "        W4_2 =   get_variable(\"W4_2\", [3,3,128,128] )\n",
    "        Z8 = tf.nn.conv2d(A7,W4_2, strides = [1,1,1,1], padding = 'SAME')\n",
    "        A8 = tf.nn.relu(Z8) \n",
    "        P4 = tf.nn.max_pool(A8, ksize = [1,2,2,1], strides = [1,2,2,1], padding = 'SAME')\n",
    "    ### Unit 5 ###\n",
    "    with tf.name_scope('Unit5'):\n",
    "        W5_1 =   get_variable(\"W5_1\", [3,3,128,256] )\n",
    "        Z9 = tf.nn.conv2d(P4,W5_1, strides = [1,1,1,1], padding = 'SAME')\n",
    "        A9 = tf.nn.relu(Z9)\n",
    "        W5_2 =   get_variable(\"W5_2\", [3,3,256,256] )\n",
    "        Z10 = tf.nn.conv2d(A9,W5_2, strides = [1,1,1,1], padding = 'SAME')\n",
    "        A10 = tf.nn.relu(Z10) \n",
    "    ### Unit 6 ###\n",
    "    with tf.name_scope('Unit6'):\n",
    "        W6_1 =   get_variable(\"W6_1\", [3,3,256,128] )\n",
    "        U1 = tf.layers.conv2d_transpose(A10, filters = 128, kernel_size = 2, strides = 2, padding = 'SAME')\n",
    "        U1 = tf.concat([U1, A8],3)\n",
    "        W6_2 =   get_variable(\"W6_2\", [3,3,128,128] )\n",
    "        Z11 = tf.nn.conv2d(U1,W6_1, strides = [1,1,1,1], padding = 'SAME')\n",
    "        A11 = tf.nn.relu(Z11)\n",
    "        Z12 = tf.nn.conv2d(A11,W6_2, strides = [1,1,1,1], padding = 'SAME')\n",
    "        A12 = tf.nn.relu(Z12)\n",
    "    ### Unit 7 ###\n",
    "    with tf.name_scope('Unit7'):\n",
    "        W7_1 =   get_variable(\"W7_1\", [3,3,128,64] )\n",
    "        U2 = tf.layers.conv2d_transpose(A12, filters = 64, kernel_size = 2, strides = 2, padding = 'SAME')\n",
    "        U2 = tf.concat([U2, A6],3)\n",
    "        Z13 = tf.nn.conv2d(U2,W7_1, strides = [1,1,1,1], padding = 'SAME')\n",
    "        A13 = tf.nn.relu(Z13)\n",
    "        W7_2 =   get_variable(\"W7_2\", [3,3,64,64] )\n",
    "        Z14 = tf.nn.conv2d(A13,W7_2, strides = [1,1,1,1], padding = 'SAME')\n",
    "        A14 = tf.nn.relu(Z14)\n",
    "    ### Unit 8 ###\n",
    "    with tf.name_scope('Unit8'):\n",
    "        W8_1 =   get_variable(\"W8_1\", [3,3,64,32] )\n",
    "        U3 = tf.layers.conv2d_transpose(A14, filters = 32, kernel_size = 2, strides = 2, padding = 'SAME')\n",
    "        U3 = tf.concat([U3, A4],3)\n",
    "        Z15 = tf.nn.conv2d(U3,W8_1, strides = [1,1,1,1], padding = 'SAME')\n",
    "        A15 = tf.nn.relu(Z15)\n",
    "        W8_2 =   get_variable(\"W8_2\", [3,3,32,32] )\n",
    "        Z16 = tf.nn.conv2d(A15,W8_2, strides = [1,1,1,1], padding = 'SAME')\n",
    "        A16 = tf.nn.relu(Z16)\n",
    "    ### Unit 9 ###\n",
    "    with tf.name_scope('Unit9'):\n",
    "        W9_1 =   get_variable(\"W9_1\", [3,3,32,16] )\n",
    "        U4 = tf.layers.conv2d_transpose(A16, filters = 16, kernel_size = 2, strides = 2, padding = 'SAME')\n",
    "        U4 = tf.concat([U4, A2],3)\n",
    "        Z17 = tf.nn.conv2d(U4,W9_1, strides = [1,1,1,1], padding = 'SAME')\n",
    "        A17 = tf.nn.relu(Z17)\n",
    "        W9_2 =   get_variable(\"W9_2\", [3,3,16,16] )\n",
    "        Z18 = tf.nn.conv2d(A17,W9_2, strides = [1,1,1,1], padding = 'SAME')\n",
    "        A18 = tf.nn.relu(Z18)\n",
    "    ### Unit 10 ###\n",
    "    with tf.name_scope('out_put'):\n",
    "        W10 =    get_variable(\"W10\", [1,1,16,1] )\n",
    "        Z19 = tf.nn.conv2d(A18,W10, strides = [1,1,1,1], padding = 'SAME')\n",
    "        A19 = tf.nn.sigmoid(Z19)\n",
    "        Y_pred = A19\n",
    "    return Y_pred\n",
    "\n",
    "def loss_function(y_pred, y_true):\n",
    "    cost = tf.reduce_mean(tf.keras.losses.binary_crossentropy(y_true,y_pred))\n",
    "    return cost\n",
    "\n",
    "def mean_iou(y_pred,y_true):\n",
    "    y_pred_ = tf.to_int64(y_pred > 0.5)\n",
    "    y_true_ = tf.to_int64(y_true > 0.5)\n",
    "    score, up_opt = tf.metrics.mean_iou(y_true_, y_pred_, 2)\n",
    "    with tf.control_dependencies([up_opt]):\n",
    "        score = tf.identity(score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build the graph as a dictionary\n",
    "def build_graph():\n",
    "    with tf.Graph().as_default() as g:\n",
    "        with tf.device(\"/gpu:0\"):\n",
    "            with tf.name_scope('input'):\n",
    "                x_ = tf.placeholder(tf.float32, shape=(None,IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "                y_ = tf.placeholder(tf.float32, shape=(None,IMG_HEIGHT, IMG_WIDTH, 1))\n",
    "            y_pred = UNet(x_)\n",
    "            with tf.name_scope('loss'):\n",
    "                loss = loss_function(y_pred,y_)\n",
    "        with tf.device(\"/cpu:0\"):\n",
    "            with tf.name_scope(\"metrics\"):\n",
    "                iou = mean_iou(y_pred,y_)\n",
    "        model_dict = {'graph': g, 'inputs': [x_, y_],'Iou':iou,'Loss':loss, 'y_pred':y_pred}\n",
    "    return model_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
